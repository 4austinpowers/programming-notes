{
  "cells": [
    {
      "metadata": {
        "_uuid": "c3cb3a0e12d89b20eb1c37ffd7094eba39caaf91",
        "_cell_guid": "07ef463a-6afc-44a9-8c75-6436e68e9eae"
      },
      "cell_type": "markdown",
      "source": "# Permutation Feature Importance \n\nOnce you've built your model and tuned your hyperparameters, there are still a few more tricks to get the most out of your model. Feature Selection is an imporant technique you should to apply to most machine learning models (deep learning is an exception) to identify features that make meaningful contributions to your model. Dropping features that are not important helps to reduce overfitting. Overfitting is discussed in another tutorial, but the gist is that if you train your model on too many features, it may become very good at fitting to the training data, but poor at generalizing to new data. \n\nPermutation Feature Importance (lets call it PFI) is slightly different from more popular feature selection techniques, like Principal Component Analysis. For PFI, you identify important features by testing an already trained model on input data that has random permutations in it, then oberseving the response. If you find that random permutations to the input data do not effect the model's performance, then it may be a less important feature. \n\nJust for review, creating a permutation of dataset is just reordering it without deleting any values. Here are all the permutations of [A, B, C]\n\n[A, B, C]\n\n[B, C, A]\n\n[C, A, B]\n\n[A, C, B]\n\n[C, B, A]\n\n[B, A, C]\n\nHere is a toy example: lets say I have a random forest regression model, using the R^2 coefficient as a metric (0-1 range where 1 is a perfect fit), and have 10 features in my data. I will iterate through each feature. For a single loop, I create create a random permuation of one of the feature vectors on a copy of the validation data. Then I will run this data through the trained Random Forest, and evaluate its R^2 performance. I then repeat this for all the features. If the original training accuracy is 0.75, and the accuracy remains 0.75 on columns 1, 3, 5, then those features are not useful to the predive power of the model, and we can try dropping them. \n\nPFI is mostly used in random forest models, but can be extended to other classification and regression models. \n\nLets now see an example with the Melbourne Housing Dataset from the previous tutorials. In the next few cells, I will load, clean, and build a Random Forest Regressor baseline model."
    },
    {
      "metadata": {
        "_uuid": "f94b605e23f480eea45e899d5d05ca9419f48b1a",
        "collapsed": true,
        "_cell_guid": "e51afb1a-4f2c-487f-883f-1048b3fa4dc9",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd \nimport numpy as np\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.ensemble import RandomForestRegressor",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4289a4c579dd74a9664900834f841fc0dfc689fb",
        "collapsed": true,
        "_cell_guid": "d498ccbd-0ee4-41f6-8290-5fadf47e922b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#read in the data \ndata = pd.read_csv('../input/melb_data.csv')\n\n#split the X and Y data \nmelbourne_predictors = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n                        'YearBuilt', 'Lattitude', 'Longtitude']\nX = data[melbourne_predictors]\ny = data['Price']\n\n#fill in missing NaN values (see Handling Missing Values tutorial)\nmy_imputer = Imputer()\nX_impute = my_imputer.fit_transform(X)\n\n#create a training and validation split \ntrain_X, val_X = X_impute[:15000], X_impute[15001:]\ntrain_y, val_y = y[:15000], y[15001:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7faab766d5608e968e5a07423d6e58fb37e9177b",
        "_cell_guid": "0ac0c143-4e72-484c-915f-9a78978ac6c4",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "forest_model = RandomForestRegressor() #build a regression model \nforest_model.fit(train_X, train_y) #train the base model \nbase_score = forest_model.score(val_X, val_y) #get a accuracy score \nprint ('Base model R^2:, ', base_score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "09dfd3d6534a37a3a5e28b542f8affb2f40783b5",
        "_cell_guid": "c7a08e35-c721-4f1d-8d98-40d6663c4345"
      },
      "cell_type": "markdown",
      "source": "The baseline model performed with an R^2 value of about 0.64. Now, we can use Permutation Feature Importance to identify which feaatures contributed most to this score. "
    },
    {
      "metadata": {
        "_uuid": "06a596183029812c651ac369322e5f5794239bf6",
        "_cell_guid": "0456a4ac-3fde-47ac-9226-6b49b3b7bcf1",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "for i in range(val_X.shape[1]): \n    \n    #don't overwrite the validation data! \n    val_X_PFI = val_X.copy() \n    \n    #create a random permutation with numpy \n    val_X_PFI[:, i] = np.random.permutation(val_X_PFI[:, i]) \n    \n    #recompute the R^2 score\n    score = forest_model.score(val_X_PFI, val_y)  \n    print ('Permute {} R^2: {}'.format(X.columns[i], score))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a1c83ce2ed143d91394e17450f00050ac2e02077",
        "_cell_guid": "194c9133-8da7-4885-b320-4aa8a85609b7"
      },
      "cell_type": "markdown",
      "source": "The results above are insightful. The results may be slightly different when you run this, due to the random nature of the calculations. We can see that the R^2 coefficient remains within 0.03 of 0.64 for the Bathroom and BuldingArea features, and within 0.10 for Landsize and YearBuilt categories. This means, that these features did not impact the predictive power of the model to a significant degree. \n\nWe can try dropping these four features and redoing the analysis. "
    },
    {
      "metadata": {
        "_uuid": "9ab47790db71ac631c1aa2dd54064ed8e248d9c5",
        "_cell_guid": "ce06d2e7-040c-474c-b035-1ed09b5ff68c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "melbourne_predictors = ['Rooms', 'Lattitude', 'Longtitude']\nX = data[melbourne_predictors]\ny = data['Price']\n\n#fill in missing NaN values (see Handling Missing Values tutorial)\nmy_imputer = Imputer()\nX_impute = my_imputer.fit_transform(X)\n\n#create a training and validation split \ntrain_X, val_X = X_impute[:15000], X_impute[15001:]\ntrain_y, val_y = y[:15000], y[15001:]\n\nforest_model = RandomForestRegressor() #build a regression model \nforest_model.fit(train_X, train_y) #train the base model \nbase_score = forest_model.score(val_X, val_y) #get a accuracy score \nprint ('New R^2:, ', base_score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "da7598468d1dface9308dd2f5d6659bc74c3dcdf",
        "_cell_guid": "87d20d14-ec31-4ae3-ac6c-e0bfa32f8235"
      },
      "cell_type": "markdown",
      "source": "As you can see, the R^2 only drops slighly with this large reduction in dimensionality. This may reduce the predictive power of this particular model, but in datasets with more features, or more complicated models, you may see better gains.\n\n# Conclusion \nFeature Selection with Permutation Feature Importance is a great way to reduce dimensionality of your dataset and help reduce overfitting. Here is a checklist of things to keep in mind when implementing PFI: \n\n- apply PFI on a trained model, using validation data\n- make a copy of you validation every iteration to not overwrite \n- use the same perfomance metric for all comparisons \n\n# Your Turn \n\nSklearn does not come with an implementation for PFI. See if you can extend this example for other performance metrics and model types (classification vs. regression). \n\n### References \n\n[Interpretable Machine Learning - Chrstoph Molnar](https://christophm.github.io/interpretable-ml-book/permutation-feature-importance.html)\n\n[pjh2011's Github /rf_perm_feat_import\n](https://github.com/pjh2011/rf_perm_feat_import)"
    },
    {
      "metadata": {
        "_uuid": "bf12bca1fd2f2bfbad41b25fffe0911c402211f3",
        "collapsed": true,
        "_cell_guid": "455c7192-c831-403d-9447-be23d5cba102",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "pygments_lexer": "ipython3",
      "name": "python",
      "version": "3.6.4",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "nbconvert_exporter": "python",
      "file_extension": ".py",
      "mimetype": "text/x-python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}