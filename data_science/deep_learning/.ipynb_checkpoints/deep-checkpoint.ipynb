{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Notes \n",
    "\n",
    "Assuming some knowledge already, this is more of a note taking space for me. \n",
    "\n",
    "## Loss Functions \n",
    "\n",
    "Loss functions are a metric for the network's performance and come in a variety of flavors for different purposes. The core purpose is the Loss Function is to measure the distance between the ground truth and the model's outputs. Depending on the type of problem you want to solve, you will chose a specific loss function. Here are the most common ones: \n",
    "\n",
    "| **Loss Function**             | **Purpose**                | **Keras**                      |\n",
    "|---------------------------|------------------------|----------------------------|\n",
    "| **Binary Cross-Entropy**      | Binary Prediction      | `binary_crossentropy`      |\n",
    "| **Categorical Cross-Entropy** | Multi-class Pred - OneHot | `categorical_crossentropy` |\n",
    "| **Sparse Categorical Cross-Entropy** | Multi-class Pred - Int | `sparse_categorical_crossentropy` |\n",
    "| **Mean Squared Error**        | Continious Regression  | `mean_squared_error`       |\n",
    "| **Cosine Proximity**        | Vector Oreintation  | `cosine_proximity`       |\n",
    "\n",
    "\n",
    "Cross-Entropy is a quantity from the field of information theory that measures the distance between probablity distributions (GT vs. output). This is good for models that output probablitites. In the case of Categorical Cross Entropy, it is imperative to one-hot-encode the data. Sparse Categorical Cross Entropy avoids this need and takes in intger values alone. \n",
    "\n",
    "Mean-Squared-Error measures the distance between two quantities (residuals) into a sort of average. Sum-of-Squared-Errors (SSE) is another option, though this can explode more easily. Root Mean Squared Error (RMSE)is just the square root of the mean square error. That is probably the most easily interpreted statistic, since it has the same units as the quantity plotted on the vertical axis for a linear regression. \n",
    "\n",
    "Cosine Proximity, or Cosine Similary, is a measure of how close two vectors are in terms of orientation, and not magnitude. This is useful in models like Word2Vec. \n",
    "\n",
    "More information on the [math here](https://isaacchanghau.github.io/2017/06/07/Loss-Functions-in-Artificial-Neural-Networks/), or in the [Keras Docs](https://keras.io/losses/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics \n",
    "\n",
    "In addition to loss there are other metrics to validate the training of a network. Metrics differ for cclassification and regression problems. \n",
    "\n",
    "### Accuracy \n",
    "\n",
    "Accuracy (acc) is the simple percentage of correct predictions for a classification problem. \n",
    "\n",
    "### Mean Absolute Error: \n",
    "\n",
    "Mean Absolute Error (MAE) is the absolute value of the difference between the predictions and the targets. It can only be used for regression. It can be interpreted as how off you are in a one-to-one comparison with the scale units of your target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optmizers \n",
    "\n",
    "In general, it safe to start with RMSprop (`rmsprop`), whatever the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions \n",
    "\n",
    "### Softmax \n",
    "\n",
    "Since softmax functions output a probablity distriubtion over many categories, you need to be sure to format the last layer correctly. The example below shows the right format for a 10-class classification with a softmax output. The model will output a 10-dim vector of probablties, whos sum is one. \n",
    "\n",
    "```python \n",
    "modeladd(layers.Dense(10, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is my Neural Net not Working? \n",
    "\n",
    "Essential Checklist: \n",
    "####  0. You're not documenting your process! C'mon! Be a scientist! \n",
    "\n",
    "####  1. Does the last layer of the network have (N nodes == N classes)? \n",
    "\n",
    "####  2. Are you using the right loss function? (check above)\n",
    "- are you one-hot encoding? (if so don't use sparse categorical cross entropy) \n",
    "- MSE for regression \n",
    "\n",
    "####  3. Are you using the right activation on the last layer?(check above)\n",
    "- no activation for regression \n",
    "- softmax for probablities \n",
    "- sigmoid [0, 1]\n",
    "- ReLU [0, inf]\n",
    "\n",
    "#### 4. You're network is too big for your data \n",
    "- scale down the number of hidden layers and nodes \n",
    "\n",
    "#### 5. You did not shuffle your data \n",
    "\n",
    "#### 6. You did not normalize your data \n",
    "- data should be between [0, 1]\n",
    "- your data could be heterogenous, where features have different scales \n",
    "    - normalize independently on a per-feature basis \n",
    "    - mean = 0, std = 1\n",
    "    - `x -= x.mean(axis=0`\n",
    "    - `x /= x.std(axis=0)`\n",
    "\n",
    "#### 7. Can you reduce the dimensionality of your data \n",
    "- feature engineering! \n",
    "\n",
    "#### 8. You're scaling up wrong \n",
    "- You are applying regularization and increasing network size at the wrong time \n",
    "- The ideal workflow is iterative, like tightening a car wheel: \n",
    "        **start with a small, basic network**\n",
    "        --> train, work out kinks, graident check \n",
    "        while Tuning: \n",
    "            --> overfit this model (add layers, nodes, epochs, gridsearch)\n",
    "            --> add regularization (L1/L2, dropout, remove layers)\n",
    "            --> work out kinks\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Snippets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the Training and Validation Loss ### \n",
    "## assumes: ##\n",
    "# import matplotlib.pyplot as plt \n",
    "# history = model.fit(...)\n",
    "\n",
    "metric = 'loss' #acc\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['{}'.format(metric)]\n",
    "val_loss_values = history_dict['val_{}'.format(metric)]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training {}'.format(metric))\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation {]}'.format(metric))\n",
    "plt.title('Training and Validation {}'.format(metric))\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('{}'.format(metric))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation \n",
    "\n",
    "In the case of small training data, you can try this: Apply K-fold multiple times, shuffling the data every time before splitting it k-ways. The final score is the average of all the scores obtained at each run of the k-fold. So you run the model (P * K) times, where P is the number of iterations of K-folds! Yes, its expensive... Psudeocode below\n",
    "\n",
    "```python \n",
    "P = 5\n",
    "K = 5\n",
    "scores = []\n",
    "for p in range(P): \n",
    "    np.random.shuffle(data) #shuffle after each k-fold \n",
    "    for k in range(K): \n",
    "        #...k-fold data split \n",
    "        model.train(train_data) \n",
    "        scores.append(mode.evaluate(val_data))\n",
    "np.average(scores)\n",
    "```\n",
    "\n",
    "An imlementation of just the k-fold can be found below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normal K-Fold Cross Validation ###\n",
    "## assumes ##\n",
    "# build_model() #function to build model\n",
    "# import numpy as np \n",
    "# withhold test data\n",
    "\n",
    "k = 4 \n",
    "num_validation_samples = len(data) // k\n",
    "np.random.shuffle(data) #inplace \n",
    "\n",
    "validation_scores = []\n",
    "for fold in range(k): \n",
    "    start_split = num_validation_samples * fold\n",
    "    end_split = num_validation_samples * (fold + 1)\n",
    "    \n",
    "    #grab the data \n",
    "    val_data = data[start_split:end_split] #validation split\n",
    "    train_data = data[:start_split] + data[end_split:] #train split \n",
    "    \n",
    "    #train & validate new instance of the model \n",
    "    model = build_model() \n",
    "    history = model.train(train_data)\n",
    "    val_score = model.evaluate(val_data)\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "val_score = np.mean(val_scores)\n",
    "\n",
    "model.get_model()\n",
    "model.train(data) \n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
