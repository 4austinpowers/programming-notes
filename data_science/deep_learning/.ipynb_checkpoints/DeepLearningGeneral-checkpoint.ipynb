{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Notes \n",
    "\n",
    "Assumes some knowledge already, this is more of a note taking space for me. \n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "# Loss Functions \n",
    "\n",
    "Loss functions are a metric for the network's performance and come in a variety of flavors for different purposes. The core purpose is the Loss Function is to measure the distance between the ground truth and the model's outputs. Depending on the type of problem you want to solve, you will chose a specific loss function. Here are the most common ones: \n",
    "\n",
    "| **Loss Function**             | **Purpose**                | **Keras**                      |\n",
    "|---------------------------|------------------------|----------------------------|\n",
    "| **Binary Cross-Entropy**      | Binary Prediction      | `binary_crossentropy`      |\n",
    "| **Categorical Cross-Entropy** | Multi-class Pred - OneHot | `categorical_crossentropy` |\n",
    "| **Sparse Categorical Cross-Entropy** | Multi-class Pred - Int | `sparse_categorical_crossentropy` |\n",
    "| **Mean Squared Error**        | Continious Regression  | `mean_squared_error`       |\n",
    "| **Cosine Proximity**        | Vector Oreintation  | `cosine_proximity`       |\n",
    "\n",
    "\n",
    "**Cross-Entropy** is a quantity from the field of information theory that measures the distance between probablity distributions (GT vs. output). This is good for models that output probablitites. In the case of Categorical Cross Entropy, it is imperative to one-hot-encode the data. Sparse Categorical Cross Entropy avoids this need and takes in intger values alone. \n",
    "\n",
    "**Mean-Squared-Error** measures the distance between two quantities (residuals) into a sort of average. Sum-of-Squared-Errors (SSE) is another option, though this can explode more easily. Root Mean Squared Error (RMSE)is just the square root of the mean square error. That is probably the most easily interpreted statistic, since it has the same units as the quantity plotted on the vertical axis for a linear regression. \n",
    "\n",
    "**Cosine Proximity**, or Cosine Similary, is a measure of how close two vectors are in terms of orientation, and not magnitude. This is useful in models like Word2Vec. \n",
    "\n",
    "More information on the [math here](https://isaacchanghau.github.io/2017/06/07/Loss-Functions-in-Artificial-Neural-Networks/), or in the [Keras Docs](https://keras.io/losses/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "#### Other Metrics \n",
    "\n",
    "In addition to loss there are other metrics to validate the training of a network. Metrics differ for classification and regression problems. \n",
    "\n",
    "**Accuracy** (acc) is the simple percentage of correct predictions for a classification problem. \n",
    "\n",
    "**Mean Absolute Error** (MAE) is the absolute value of the difference between the predictions and the targets. It can only be used for regression. It can be interpreted as how off you are in a one-to-one comparison with the scale units of your target. \n",
    "\n",
    "### Important Consideration \n",
    "\n",
    "You may find that at some points, your accuracy increases while loss stays the same or increases. How is this possible? Well, the loss displayed is an average of pointwise loss values, but accuracy is a threshold of the class prediction probablities. The number of correctly classified points may be increasing (better accuracy) but this may not be seen in the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "# Optmizers \n",
    "\n",
    "In general, it safe to start with RMSprop (`rmsprop`), whatever the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "# Activation Functions \n",
    "\n",
    "[CS231](https://cs231n.github.io/neural-networks-1/#actfun)\n",
    "\n",
    "TLDR: “What neuron type should I use?” For middle layers, ese the eLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this does not work, try ReLU, maxout, or tanh (worst case). \n",
    "\n",
    "For the final layer, this depends on what you want to predict. For probablities, softmax. For regression, no activation. For values [0, 1] (binary classification) sigmoid. \n",
    "\n",
    "<img src=\"img/activations.png\" width=\"600\" />\n",
    "\n",
    "### Sigmoid\n",
    "The sigmoid non-linearity has the mathematical form σ(x)=1/(1+e−x); it takes a real-valued number and “squashes” it into range between 0 and 1.Large negative numbers become 0 and large positive numbers become 1.  In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks: saturation to 0 gradient at the tails, and being non-zero centered. \n",
    "\n",
    "### Tanh \n",
    "\n",
    " Tahn squashes values between [-1, 1]. Tanh non-linearity is always preferred to the sigmoid nonlinearity since it is 0 centered, but the gradients are still 0 at the tails. \n",
    " \n",
    " ### ReLU \n",
    "The Rectified Linear Unit has become very popular. It computes the function `f(x)=max(0,x)`; the activation is simply thresholded at zero. It is inexpensive, but the activations can \"die\" at high gradients\n",
    "\n",
    "### Leaky ReLU \n",
    "\n",
    "Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when x < 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes `f(x)=1(x<0)(αx)+1(x>=0)(x)` where α is a small constant. \n",
    "\n",
    "### eLU \n",
    "ELU(Exponential linear unit) function takes care of the Vanishing gradient problem. \n",
    "Now what ELU does is that it tries to make the mean activation close to zero and as it is an exponential function it does not saturate. This behavior helps to push the mean activation of neurons closer to zero which is beneficial for learning and it helps to learn representations that are more robust to noise.\n",
    "\n",
    "### MaxOut \n",
    "The Maxout neuron computes the function `max(wT1x+b1,wT2x+b2)`. Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have w1,b1=0). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters.\n",
    "\n",
    "### Softmax \n",
    "\n",
    "This is a soft/smooth approximation of max. Softmax functions output a probablity distriubtion over many categories.  The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "# Why is my Neural Net not Working? \n",
    "\n",
    "You can follow these two guides: [this simple one](http://theorangeduck.com/page/neural-network-not-working), and [Andrej Karpathy's from CS231n](https://cs231n.github.io/neural-networks-3/#gradcheck)\n",
    "\n",
    "## Before Anything: \n",
    "- try to visualize your results\n",
    "- try to visualize the data before it goes into the network \n",
    "- train for a few more epochs to make sure things aren't working \n",
    "\n",
    "## Gradient Checks: \n",
    "Do this stuff after you design your network, but before real training. \n",
    "\n",
    "#### Loss with no Regularization \n",
    "- Turn off all regularization and check the data loss \n",
    "- ex.  for CIFAR-10 with a Softmax classifier we would expect the initial loss to be 2.302\n",
    "    -  we expect a diffuse probability of 0.1 \n",
    "    -  10 classes, and Softmax loss is the negative log probability of the correct class\n",
    "    - so: -ln(0.1) = 2.302.\n",
    "\n",
    "#### Increase Regularization \n",
    "- now, increasing regularization should increase the loss \n",
    "\n",
    "#### Overfit on a subset \n",
    "- Overfit on a small subset of the data (20 examples)\n",
    "- Ensure you can get 0 loss \n",
    "- If you don't pass, something is wrong \n",
    "\n",
    "## Essential Checklist: \n",
    "\n",
    "####  0. You're not documenting your process! C'mon! Be a scientist! \n",
    "\n",
    "#### 1. You did not shuffle your data \n",
    "\n",
    "#### 2. You did not normalize your data \n",
    "- data should be between [0, 1]\n",
    "- your data could be heterogenous, where features have different scales \n",
    "    - normalize independently on a per-feature basis \n",
    "    - mean = 0, std = 1\n",
    "    - `x -= x.mean(axis=0`\n",
    "    - `x /= x.std(axis=0)`\n",
    "\n",
    "####  3. Does the last layer of the network have (N nodes == N classes)? \n",
    "\n",
    "####  4. Are you using the right activation on the last layer?(check above)\n",
    "- no activation for regression \n",
    "- softmax for probablities \n",
    "- sigmoid [0, 1]\n",
    "- ReLU [0, inf]\n",
    "\n",
    "####  5. Are you using the right loss function? (check above)\n",
    "- are you one-hot encoding? (if so don't use sparse categorical cross entropy) \n",
    "- MSE for regression \n",
    "\n",
    "#### 6. Are you regularizing? \n",
    "- dropout, start with 0.9 and work down to 0.5\n",
    "- L1/L2\n",
    "- data augmentation \n",
    "\n",
    "#### 7. You are using the wrong learning rate \n",
    "- Which of the following does your loss look like?\n",
    "- Low learning rates the improvements will be linear.\n",
    "- With high learning rates they will start to look more exponential. \n",
    "- Higher learning rates will decay the loss faster, but get stuck (green line).\n",
    "    - This is because there is too much \"energy\" in the optimization and the parameters are bouncing around chaotically, unable to settle in a nice spot in the optimization landscape.\n",
    "<img src=\"img/learningrates.jpeg\" width=\"240\" />\n",
    "\n",
    "#### 8. You're using the wrong batch size \n",
    "- low batch size (1-16) increases stochasticity, noisy updates \n",
    "- choppiness can \"jump\" out of local minima\n",
    "- increases training time \n",
    "- chart shows good LR, noisy udpate implies low batch size \n",
    "<img src=\"img/loss.jpeg\" width=\"240\" />\n",
    "\n",
    "#### 9. Can you reduce the dimensionality of your data \n",
    "- feature engineering! \n",
    "\n",
    "#### 10. You're network is too big for your data \n",
    "- deeper != better \n",
    "- scale down the number of hidden layers and nodes \n",
    "- start with 3-8 layers, 256-1024 nodes, go deeper after sucess \n",
    "\n",
    "#### 11. You're scaling up wrong \n",
    "- You are applying regularization and increasing network size at the wrong time \n",
    "- The ideal workflow is iterative, like tightening a car wheel: \n",
    "        **start with a small, basic network**\n",
    "        --> train, work out kinks, graident check \n",
    "        while Tuning: \n",
    "            --> overfit this model (add layers, nodes, epochs, gridsearch)\n",
    "            --> add regularization (L1/L2, dropout, remove layers)\n",
    "            --> work out kinks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
