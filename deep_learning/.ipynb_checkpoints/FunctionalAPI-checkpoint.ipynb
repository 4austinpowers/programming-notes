{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Functional API\n",
    "\n",
    "For single-input single-output models, the Sequentual Keras API will suffice. But for more complicated models, where there can be many inputs, outputs. For example you may need to predict the price of a product from its image, description, and metadata. Or you may need to get the genre and date from a movie synopsis. You may even have internally complex models with nonlinear, like U-Nets or Inception modules. All these need a different API. \n",
    "\n",
    "<img src=\"img/inception.png\", width=\"300\"/>\n",
    "Inception Module \n",
    "<img src=\"img/unet.png\", width=\"500\"/>\n",
    "UNet for Semantic Segmentation\n",
    "\n",
    "The fuctional API allows you to use Keras layers like functions, and pass in other layers as input. You can design one quite simply: \n",
    "\n",
    "```python \n",
    "x = layers.Dense(input = (var)) #define a layer  \n",
    "x2 = layers.Dense(32)(x) #new func, feed in the last layer\n",
    "x3 = layers.Dense(10)(x2) #new func, feed in the last layer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of a 3 output model for a CNN: \n",
    "\n",
    "```python \n",
    "text_inputs = Input(shape=(None, ), name='text') #input \n",
    "embedding = layers.Embedding(256, vocab_size)(text_inputs) #embedding \n",
    "x = layers.LSTM(256)(embedding) #network - recursive \n",
    "x = layers.LSTM(256)(x)\n",
    "x = layers.LSTM(256)(x)\n",
    "x = layers.LSTM(256)(x)\n",
    "x = layers.LSTM(256)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "#create predictions \n",
    "age_prediction = layers.Dense(1, activation = None, name = 'age')(x)\n",
    "income_prediction = layers.Dense(1, activation = 'softmax', name = 'income')(x)\n",
    "gender_prediction = layers.Dense(1, activation = 'sigmoid', name = 'gender')(x)\n",
    "\n",
    "#build, declare IO\n",
    "model = Model(inputs = text_inputs, \n",
    "             outputs = [age_prediction, income_prediction, gender_prediction]) \n",
    "```\n",
    "\n",
    "But this alone will cause an issue: the losses are imbalanced. There is a regression (age), and both binary and multiclass classifications. Each requres a different loss functions, and those loss functions will have different scales. Since gradient descent forces you to minimize a scalar, you must created a [weighted] average of the losses. \n",
    "\n",
    "There are two ways to do this. As a list, or dict (names required).\n",
    "\n",
    "```python \n",
    "model.compile(optmizer='rmsprop', \n",
    "             loss = ['mse', 'categorical_crossentropy', 'binary_crossentropy'], \n",
    "             loss_weights = [0.25, 1., 10.])\n",
    "```\n",
    " \n",
    " OR \n",
    " \n",
    "```python \n",
    "model.compile(optmizer='rmsprop', \n",
    "             loss = {'age' : 'mse'\n",
    "                     'income' : 'categorical_crossentropy',\n",
    "                     'gender' : 'binary_crossentropy'}, \n",
    "             loss_weights = {'age' : 0.25\n",
    "                             'income' : 1.,\n",
    "                             'gender' : 10.})\n",
    "```\n",
    "\n",
    "The loss balance is another parameter to tune. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connections \n",
    "\n",
    "For CNNs, residuals connections are an importnat area for layer reuse. They prevent a common issue with deep networks. When network depth increases, accuracy gets saturated and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error (vanishing gradients - loss of gradient signal in a deep network makes it untrainable). \n",
    "\n",
    "Residual Connections consits of feeding the output of an old layer available as input to a later layer, creating a shortcut in a squential network. The old output is then summed with this output. This is sort of like an LSTM's layer reuse, except that they are purely linear. \n",
    "\n",
    "Residual connections are good for networks with larger than 10 layers. \n",
    "\n",
    "```python \n",
    "x = ... #4d input tensor \n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.add([x, y]) #sum both layers \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Sharing \n",
    "\n",
    "Layer sharing is another good way to save computation for shared processing. An example is when you need to compare two sentences for similarity. The LSTM could learn single representation for a sentence (siamese LSTM). \n",
    "\n",
    "```python \n",
    "lstm = layers.LSTM(32) #define one LSTM \n",
    "\n",
    "left_input = Input(shape=(None, 128))\n",
    "left_output = lstm(left_input)\n",
    "\n",
    "right_input = Input(shape=(None, 128))\n",
    "right_output = lstm(right_input)\n",
    "\n",
    "merged = layers.concatenate([left_output, right_output], axis=-1)\n",
    "predictions = layers.Dense(1, 'sigmoid')(merged)\n",
    "\n",
    "model = Model([left_input,right_input], predictions)\n",
    "model.fit([left_data, right_data], targets)\n",
    "```\n",
    "\n",
    "Here is a siamese CNN example. This can be used for sharing the convolutional base (learning image representations) between two closely placed cameras that together, sense depth. \n",
    "\n",
    "```python \n",
    "xception_base = applications.Xception(weights=None, include_top = False) #conv base \n",
    "\n",
    "left_input = Input(shape=(250, 250, 3))\n",
    "right_input = Input(shape=(250, 250, 3))\n",
    "\n",
    "left_features = xception_base(left_input)\n",
    "right_features = xception_base(right_input)\n",
    "\n",
    "merged = layers.concatenate([left_features, right_features], axis=-1)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
