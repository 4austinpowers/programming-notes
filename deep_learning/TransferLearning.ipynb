{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning \n",
    "\n",
    "Transfer Learning consists of taking a saved network that was trained on a larage dataset, and reusing sections for other purposes. If the saved network is large and general enough, then its spatial hieracrchy of features can be transferred to new tasks. \n",
    "\n",
    "VGG is a common model to be used for image classification tasks. You can take the convolitional base layers, freeze them so as to not be retrained, and rebuild the flattened Dense layers for training. \n",
    "\n",
    "Available models for Keras are: Xception, Inception V3, ResNet50, VGG16, VGG19, MobileNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohit\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 42s 1us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16 \n",
    "\n",
    "conv_base = VGG16(weights='imagenet', \n",
    "                   include_top=False, \n",
    "                 input_shape=(150, 150, 3)) #optional param! \n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point there are two options to continue training. One is significantly cheaper computationally, while the other is cleaner in terms of code and allows you to use data augmentation and other. \n",
    "\n",
    "----------------------------------------------------------------\n",
    "\n",
    "### Training Option 1: Feature Extraction \n",
    "\n",
    "The first option is to use the convolutional base (or other pretrained classifier) as a feature extractor, resave the data, then feed that new data into a new Dense network. This is essentially the same as a normal network, except you are breaking up the network and saving data in the middle. You need to build a new model of the flattend Dense layer and train that. Because it is just a feed-forward NN, it is quick and can be trained on a CPU, but still preferbly a GPU. Here is some psuedo code. You would probably use a generator for this in the real world. \n",
    "\n",
    "```python \n",
    "def extract_features(x_data, feed_size):\n",
    "    features = np.zeros(shape=(feed_size, 4, 4, 512)) #size of conv_base output\n",
    "    for i in x_data: \n",
    "        batch = conv_base.predict(x_data[i]) #predict each sample with VGG\n",
    "        features[i, :, :, :] = batch #store in features \n",
    "   return features #return predictions for all your input data \n",
    "\n",
    "train_features, train_labels = extract_features(train_data, 1000), y_data\n",
    "val_features...\n",
    "test_features...\n",
    "```\n",
    "\n",
    "Next, you can build a feed-forward NN as normal, where the input data is the output from the conv_base prediction above. \n",
    "\n",
    "```python \n",
    "model = Sequential()\n",
    "model.add(layers.Dense(256, 'relu', input_shape=(4*4*512)))\n",
    "model.add(layers.Dense(1, 'sigmoid'))\n",
    "model.fit(train_features, train_labels...) #output from conv_base\n",
    "```\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "\n",
    "### Training Option 2: Train as Normal \n",
    "\n",
    "The second option is much cleaner in terms of code, but expensive (only attempt with GPU). This is because there are the same amount of parameters as there would be if training the network from scratch, and VGG is big. The benefit here is that you can use data augmentation or add other peripherals to the network with much more ease. \n",
    "\n",
    "It is important to set the conv_base weights parameter for trainable to False. Otherwise, you are destroying the previously learned representations. \n",
    "\n",
    "```python \n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "conv_base.trainable = False ###IMPORTANT ###\n",
    "model.add(layers.Dense(256, 'relu', input_shape=(4*4*512)))\n",
    "model.add(layers.Dense(1, 'sigmoid'))\n",
    "model.fit(x_data, y_data...) #train as normal\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Fine Tuning \n",
    "\n",
    "You can also fine tune a few layers of VGG which may help learning. You will want to only train the conv-block near the end of the network, right before the Dense layers. These end layers are mroe specialized features, worth tuning. The early layers only create representations for general features that are present in all images. You can set specific layers to trainable as so: \n",
    "\n",
    "```python \n",
    "for layer in conv_base: \n",
    "    if 'block5' in layer.name: \n",
    "        print (layer.name)\n",
    "        layer.trainable = True \n",
    "    else: \n",
    "        layer.trainable = False\n",
    "```\n",
    "\n",
    "This will take longer to train, but you may see better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
